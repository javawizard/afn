
I just got an idea I'm thinking of somehow using in Zelden: pipelines.

These would actually only have to do with Zelden Server. They would be a python construct allowing processing of objects in multiple stages, which I'm going to refer to as a pipeline. It should be its own independent library without any dependencies on Zelden.

So, pipelines. A pipeline is a simple construct that can be used to put an object through numerous stages of processing. Each stage has an associated priority. The priority can be any object, but typically it'll be a number or, for increased precision, an instance of decimal.Decimal. All that really matters is that all priority objects used in a particular pipelines are mutually comparable.

Priorities are actually the opposite of what you might think they would be: an object being processed by the pipeline is put through the stage with the lowest priority first, then the next, and so on. Stages with equal priority process objects in an undetermined but consistent order.

Objects can be injected into the pipeline at any priority (causing only stages with an equal or higher priority to process the object). When using any sort of numerical system for managing priorities, convention is to inject objects into a pipeline with priority 0 and process from there.

Each stage is represented by a function (or any sort of callable). When an object is to be processed by that stage, the stage function is called, passing in the object to be processed. It can return one of a few things:

	Either the object itself or None. Processing of the object (with any modifications that may have been made to it) will continue with the next stage.
	
	zelden.server.pipeline.discard. Processing of the object will stop.
	
	The result of calling zelden.server.pipeline.fork. The arguments to this function are a number of objects (any of which may be the original object), each one as an argument. All of the specified objects will be processed by the next stage. If there are multiple stages after the next one, each of the specified objects will fully complete the pipeline process before the next one begins processing at the stage after the one returning the fork instruction.

When an object passes through the last stage, an optional function passed into the pipeline at construction time will be called. The general expectation is that any final steps of actually consuming an object in a pipeline will be themselves registered as stages that don't modify the object, but this optional function is provided in case other uses of the library find a use for it.

Attaching a stage to a pipeline is not a thread-safe operation, but putting an object through a pipeline is. Pipelines are designed to work in single-threaded environments where need be, and as a result they are synchronous: when the inject function is called on a pipeline (which injects an object for processing), the function does not return until the object, and any additional forked objects generated by various stages along the way, have been processed, and the stages will all be invoked on the thread calling the inject function.

TODO: default inject priority is 0, default attach priority is 1000 (possibly custom define it on a per-pipeline basis at some later point), consider allowing the list of stages to come from a function that's called every time a new object enters the pipeline to ask it for the list of stages, or have the function be one where you ask it for the next stage that the specified object should be put through given the last stage, although that might be getting a bit complicated and that could be emulated using generators if we just go with the first approach of returning an ordered list of stages given the object's entering priority or something. Actually, I think the ordered list is best, particularly because if a stage forks then we need to be able to figure out the next stage for several different objects which messes generators up. So I'll do it with just a list-returning function, or perhaps a class implementing StageManager or something, which then it could have a function allowing stages to be added to it if it supports such an idea, which the default StageManager would. The pipeline attach function would just delegate to that then.

So I'm thinking there should also be an attach_before function. attach would be defined to add stages to the end of the list of all stages with the same priority, while attach_before would be defined to add stages to the beginning of the list of all stages with the same priority. The backing StoreManager would have to support both of these functions; it would be free to raise an exception if it didn't support modifying the list of stages or if it didn't support one of these operations.

Pipelines should also have a chain function. You pass another pipeline into the chain function, and it sets the local pipeline's consumer function to be a function that injects objects into the specified pipeline with an optionally-specified priority.



























 