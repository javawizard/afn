Working on an archive file format, like git's pack files...

So, how they'll work.

Actually, first I'm going to go have a look at how git does its index files...

(I want, if possible, Filer's archive format to store the index and the data in the same file)

So it looks like they're using a B-tree-like thing, only not. What they're doing is storing a table of offsets into a location table, more or less, as a table whose keys are the first bytes of the SHA1 hashes. So it's like a B-tree but with only one level.

Which makes me wonder if I couldn't do a B-tree-like thing, but with multiple levels.

So, have the upper four bits of the first byte of the SHA1 form one key, and the lower four bits form the next key, and so on until only, say, sixteen commits remain, and then just use those.

Actually no, let's do 256-entry tables for now, to give less disk reads to look up a single file in a pack.

No, never mind, let's make the table size dynamic, so that it can be specified when generating the index part of the pack file.

So, let's say that the table size is N and the maximum number of commits before a table is made is M.

If there are less than (or there are exactly) M commits with a certain prefix, then those commits are stored in order.

Otherwise, those commits are made into a table with N buckets, and the process is repeated to the prefix specified by the entries in those buckets.

That, of course, means that N must be a power of two, which is fine.

Actually, let's just say that M and N are 256 for now. We can adjust them later.

You know, maybe we should have separate index files, because it's pointless to send the index data across the network.

So in that case, I'll write the data file format first, then come back to the index file format later.

So, data files are stored with a magic number, which I'll decide on later.

Then they have an eight-byte number specifying how many objects they contain.

(Objects can be any binary data; they need not be BEC objects, although they pretty much always will if they come from Filer.)

Then each object is present, such that, for any object B which is stored as a diff of object A, A comes before B in the file.

Each object is encoded starting with the object's hash, stored as a sequence of 20 bytes. (There's a chance I may use SHA-256 in the future, in which case this will become 32 bytes.) Then comes a single byte, specifying how the object is encoded. (This is called the type code.) Then eight bytes follow, specifying the length of the object's encoded data. (I may change this to be some sort of variable-length encoding later.) The object's encoded data then follows, based on the particular type code:

    \x01: The object's data is stored literally, byte-for-byte. No compression, nothing.
    
    \x02: The object's data is stored as-is, but compressed with gzip.
    
    \x03: The object's data is stored as a bsdiff4 delta of a previously-encoded object. The first 20 bytes (or 32 bytes, if SHA-256 hashes end up being used) specify the hash on which this object is based. The rest of the bytes are the diff.
    
    \x04: The object's data is stored as a gzipped bsdiff4 delta of a previously-encoded object. The format is the same as for \x03, but the bsdiff4 data is stored gzipped.



That's it for the format for now. I might store a hash of the file at the end later on, but not yet.

So, now for how the archiver itself works.

What it does is takes an iterator of (hash, fileutils.File) tuples and wraps it in a mutex to allow thread-safe requests for the next object to encode.

Then it spawns a number of threads (the precise number can be specified) that sit there and request new objects from the iterable. Each thread, on requesting a new object, obtains a lock, requests the object, adds it to the end of the sliding diff window, removes the object at the other end of the sliding window if it's at the maximum length, makes a copy of the sliding window, then releases the mutex. The sliding window includes the hashes and file objects of all of the input files; the thread then proceeds to generate a diff of the file it's processing against all of the files in the sliding window that it saw when it released the mutex. It then makes gzipped copies of the diffs, and then it makes a gzipped copy of its original data.

Then it checks to see which, of all of the combinations of diffs, gzipped diffs, original data, and gzipped original data, is the smallest. It then obtains the file output lock, writes a new entry at the end of the file (a pointer to where the last thread that wrote to the file left off is stored in memory), and releases the lock. It then requests another object to diff and starts all over.

Once all of the threads are done, the process is complete.

The function that runs and consumes objects to process is written as a function that can just be called directly from the archiver instead of in a thread, if the archiver is to be run with threads disabled. There's not really much of a point to that, but you can do it if you want.



So it sort of worked. I haven't got it to do threads yet; I think that'll help a lot. But it sort of works.

The main issue is that the commits are being sorted by commit hash right now, which is really less than optimal. And, I think because of that, I'm only seeing ratios along the lines of 50%. Which is obviously better than raw commits, but still not optimal.

Another observation: gzipped diffs essentially never turned out to be smaller than raw diffs, so I'm scrapping that bit for now.

So we just compare the original file, the gzipped original file, and the diffs (not gzipped) against the other files.

So, the next step, I think, is to have the iterator yield a set of files to compare against, instead of manually keeping a window.

I'll probably have the iterator yield the revision's parents for now, and that's it.

At a later date, I might have it yield a few of their parents, too, up the line until, say, 20 commits are collected.

And after I get that done, I want to have this thing keep track of how many diffs there are in a particular chain, so that it can avoid going too deep that performance suffers, and stuff.

And actually, I just realized that the parents idea is bad, because what we really need is to diff against children, but that won't work because we don't have a way to access children.

Actually, let's try sorting commits by their numeric sequence in which they were inserted in the repository first, and see if that improves things any. It won't help as much as git because the multiple-commits-per-changeset thing (a changeset is a set of commits all made at the same time with the same commit command) means a lot poorer locality of reference, but it should be better than the hash-based stuff going on right now.

Also, I might want to trim down the repository to the point just before I imported uservoices, since those being binary files are probably unnecessarily taking up a lot of space since those won't diff well.



So I just tried things without the uservoices folder, and that's a bit more like it: 86% smaller (i.e. compressed was 14% the size of the original). And I think I might have a jython.jar in there somewhere; I bet removing that would make everything even smaller.

So this could work.

I just need to make it faster. Maybe find a faster binary diff algorithm, if there's one faster than bsdiff4 out there. Or try also doing textual diffs and gzipping them (since those would benefit from gzipping), although I'd need to make sure that they can preserve binary content, too (even if they're really inefficient at storing it, since then the bsdiff would take over because it'd be smaller).

Adding support for multiple threads made it fast enough that I'm going to leave it as is for now.

So now I need to figure out how to the archive index. I've decided that I don't like the B-tree approach so well because it will result in a lot of reads. I'm thinking I might experiment with a hash-based format instead.

And the hash used would be the CRC32 of the name of the file to be archived; this is so that the archiver can be used separately from Filer, with filenames that might not themselves be hashes.

So I'm undecided as to whether to use separate chaining or open addressing. I'm leaning toward separate chaining because I'm rather more familiar with how to write separate chaining hash tables, and because it'll have better locality of reference: assuming the linked lists are stored in a separate section of the file from the hash table, only two disk reads would be needed to determine the position of a particular object in the archive: one from the corresponding cell in the hash table, and one from the linked list area.

Separate chaining it is, then.

So, the hash table will almost certainly be located at the beginning of the file, so that a single object lookup can be done without any need to seek backward in the file. I'm thinking the hash table will store indexes relative to the start of the linked list and the linked list will store indexes relative to the start of the actual object contents. This will allow for 1: the future potential of splitting out the index and the object content, and 2: the linked list table to be written without needing to know how long it will be in advance. Offsets to the start of the hash table, the start of the linked list table, and the start of the object content will be stored at the beginning of the file, probably after the magic number.

So, the format...

Somewhere at the beginning of the file will be the offset table, as mentioned before. This will have four 8-byte integers, so 32 bytes total.

    The first integer specifies the offset to the start of the hash table.

    The second integer specifies the offset to the linked list table.
    
    The third integer specifies the offset to the start of object data.
    
    The fourth integer specifies how many hash buckets were used, as a number N such that the number of hash buckets used is equal to 2**N (where ** is exponentation).

Then there's the hash table, which is simply stored as one eight-byte integer for each entry in the hash table. This eight-byte integer specifies an offset from the start of the linked list table at which the relevant linked list node starts.







