So Autobus 1 worked great. I designed it, implemented it, and wrote support for it into a lot of applications that I wrote. And all-in-all, it was awesome.

In fact, as of my writing this (2011.09.11, which also happens to be the tenth anniversary of the World Trade Center attacks), Autobus powers nearly all of the automation stuff in my house.

So what's Autobus 2 all about?

Well...

There were a few things that I realized I did wrong in the original Autobus, and a few things that were still missing.

Probably the main thing I did wrong, and the main reason for my starting Autobus 2, is that Autobus has a server that everything connects to, and this server presents a single point of failure.... hm, I just realized a problem with this. If a bus broadcasts when it starts up to figure out what other services are available, and a service is registered locally right after that, chances are that the broadcast indicating that a new local service is available is going to collide with the responses from other devices telling what services they have.

And the responses from said other devices could end up colliding with one another.

Hm, this is an interesting problem.

So I'm thinking, why don't we have all broadcasts indicating that a service is available delayed by a second (or some delay configurable when constructing the Bus that would default to a second) so that it doesn't collide with responses to the "What services are available?" query.



The use of a server also means that each Autobus client has to be configured with the server that it's supposed to connect to, so it's generally impossible for an Autobus client to just appear on a network and not have to have any configuration in order for other things on the network to work with it.

It also means that all traffic is duplicated even when there exists a direct path between the client providing an interface and the client using said interface.

(Oh, and I'm going to start referring to interfaces as services from now on, since that more accurately describes what they do. They'll be referred to as services in the Autobus 2 code as well.)

So, service discovery without knowing a server in advance is missing.

Also missing from Autobus 1 are some additional methods of communication. I've found that functions, events, and objects are all very good at what they need to do, but that there are some other things that would be useful. The two I'm thinking of right now are channels and streams.

Channels would be this thing where a service provides a named channel, and then a client decides that it wants to open a session with said channel. It can do this, at which point there's an established link between the service and the client. If either one dies, the other gets notified that the channel has been closed; the same happens if either end explicitly closes the channel. This is useful for allowing clients to communicate where one side (or both) needs to know when the other side disappears.

Channels exchange JSON objects. Either side can send objects to the other side.

I'm thinking I'll visit streams later. My idea is that they're simply streams of bytes, but these can be emulated using channels whose objects are binary blobs of data for now, so I'm probably going to skip over this for now.

So... Autobus 2 will be distributed.

There won't be a central server. Like at all.

There can be, if services from two different networks need to be bridged. In that case, the central server would simply act as a client to other Autobus-providing things.

So I need to think more about how to do this.

So we want a way to be able to get a list of Autobus services currently available to the network. This could be done by just watching for broadcasts advertizing services and tracking the list based on that, and then removing services when a broadcast stating that the service is about to disappear is received or when it's been more than a few minutes since anything was received from the service.

So now it's becoming more complicated. I need to think things through.

So I'm thinking, what if we only have a socket opened to a particular service when we actually need to do stuff with that service?

Services would broadcast their JSON info objects (which have keys like "name" and a few other fixed things, plus any service-specific stuff such as, for example, "monitor.host" for monitord) every few minutes. They would also make three or four (or some other configurable number) of broadcasts when they're going down so that other clients on the network can remove the services immediately. If a client doesn't receive a broadcast for a service for a certain number of minutes, it removes the service from its list of currently-available services. Established connections to the service are still maintained until they, too, time out.

Services can't change their JSON objects while they're live. They have to die and then come back to change their JSON objects. If an Autobus client receives a broadcast for a service with a changed JSON object, it should remove the service and then recreate it with the new JSON object. It should not destroy any already-existing connections to the service, however, as if the service has really vanished, those connections will time out on their own. (This should never happen anyway, so I'm not particularly worried about the adverse impacts.)

Ok so, that's pretty good for the service discovery protocol. And a particular client only broadcasts when it actually has services that it's providing, and it partially randomizes the time between broadcasts so as to reduce the chance of a conflict between two clients broadcasting at the same time.

So... We have something in a Bus instance that lets clients register functions to be called whenever a service appears or disappears. We have something to obtain a connection to a particular service and then do things with that connection. Services are identified uniquely by a triple consisting of the host to connect to, the port to connect to, and a string identifying the service. Connections, therefore, can be established either to a service discovered via normal UDP broadcast or to a service whose identity-triple is known beforehand; libautobus would provide functions to establish connections to either one.

(I'm thinking that hosts would provide a service named "autobus" that provides functions for asking them what other services they're providing to make it easier for things to connect to something when they only know a host and a port, but I'll add that later.)

So, we can establish connections to a particular host/port/service combination. When that happens, libautobus connects to that host/port/service and sends it the id of the service that we're connecting to. And thus begins a session with the specified service.

So let's see... I'm thinking that the user has to create a Bus instance to do anything, even if they don't want to use service discovery. They could disable the whole service discovery thing if the wanted when they create the Bus instance.

But the reason that I'm thinking a Bus instance should be created no matter what is that each Bus would start a thread which would take care of all of the networking related to the Bus itself; this thread would be the one that runs the select (or epoll) loop. All connections created from a specific bus would use its select loop.

(Autobus 2 is thread-safe just like Autobus is, if you can't tell.)

And service discovery can be disabled when a Bus is created, in which case it won't watch for service broadcasts and it won't send any broadcasts for any services locally registered with it.

But anyways... So let's see...

So, when services appear, they broadcast that they just appeared. They broadcast this a few times.

When services disappear (and they know about it, so this doesn't include when a machine just gets yanked from the network), they also broadcast a few times.

When a bus appears, it sends out a broadcast, querying the network for any services that are available on it. When it receives a response for such a query, it adds that service to its list of available services if said service is not already in the list. It also listens for broadcasts by other services indicating that they are available, and when it receives one, it adds it to the list of things.

It also listens for broadcasts indicating that a service is going away. When it receives such a broadcast, it immediately removes the service from the list of available services.

When it's been some time (this time TBD) since a broadcast was received for a particular service, Autobus tries to connect to it via a TCP socket. If it can't establish such a connection after a certain amount of time (by setting a TCP timeout of, say, 30 seconds), it removes the service from the list of available services.

The program using autobus can register listeners to be notified when services become available or go away. When registering such a listener, it can specify that the listener should also be called for every currently-available service as well, so that if, for example, the listener were used to update a UI list, the UI list would get correctly initially populated. Programs can also register both a service-is-available and a service-is-no-longer-available listener at the same time so that, in the aforementioned UI list scenario, there isn't a time gap between the add listener and the remove listener being registered during which extra, unexpected events might be received. (I'm thinking that there will just be one listener that you pass into such a dual-listener setup, which takes a boolean indicating which of the two things happened, but I can figure that out then.)

When a program decides that it wants to connect to a particular service, it can do so by calling the connect() method on a Service object; this establishes a socket connection to the specified service, throwing an exception if such a connection cannot be established, and returning a Connection instance otherwise.

Connection instances track an individual connection's state. They do not automatically reconnect, but they can be watched to see when they die. (Adding a "listen-for-dying" listener will cause the listener to be fired if the connection is already dead, if a certain parameter is specified when registering the listener.)

A particular connection instance can have functions called on it (synchronously, asynchronously without waiting for a response, and asynchronously where the response causes a particular function to be called with it or another function to be called if an exception is thrown or something else happens), events listened to, objects watched (the listeners for these objects will be called with the object's initial value, and the object's value will appear to be set to None when the connection disconnects), and such.

A connection can, of course, be manually closed.

So that takes care of that bit. Now, what about the stuff that allows connections to be reestablished?

Well, Autobus 2 allows things to move locations, obviously, so we can't have a Connection reestablish itself. Instead, there are proxies that bind to services specified by certain search criteria automatically and act as if they were those services, which provides the reconnecting behavior of Autobus 1. This also allows for proxies to be created that bind to multiple services (instead of just the first one that matches) and expose all of them; functions called on such proxies must be asynchronous (since there's not a single thing on which to call a function; it would also be specifiable what happens if the proxy isn't currently bound to any services, and I'm still debating on whether such a proxy should allow for receiving responses asynchronously or whether it should force responses not to be received), event listening is different in that the listener is notified in some way whenever any of the events on the services fire (along with which service fired the event), and object watching somehow provides a list of objects of all of the bound services and informs when a service is bound or unbound and when objects on each service update.

I haven't yet decided whether functions, events, and objects are in their own namespaces or if they share a namespace.

Oh, and single service proxies are interface-compatible with connections; it's only multiple-service proxies that aren't.







































